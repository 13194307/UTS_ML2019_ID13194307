{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/13194307/UTS_ML2019_ID13194307/blob/master/ML_A2/NeuralNetwork-backup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58CYTad3_pXS",
        "colab_type": "text"
      },
      "source": [
        "TODO LIST:\n",
        "\n",
        "\n",
        "*   Finish off backpropagation (currently only updates output layer weights)\n",
        "*   Perhaps add different kinds of layer classes (one for Dense layers, one for output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzMSrFr7rdcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import *\n",
        "from scipy.special import softmax\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAtP_X--sHAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "    class Layer:\n",
        "        class Neuron:\n",
        "            def __init__(self, inputShape):\n",
        "                self.weights, self.bias = self.initialiseWeights(inputShape)\n",
        "                \n",
        "            def initialiseWeights(self, inputShape):\n",
        "                weights = np.array([np.random.randn() for _ in range(0, inputShape)]) / math.sqrt(inputShape)\n",
        "                bias = np.random.randn()\n",
        "                \n",
        "                return weights, bias\n",
        "            \n",
        "            def getWeights(self):\n",
        "                return self.weights\n",
        "            \n",
        "            def generateNeuronOutput(self, x):\n",
        "                return np.dot(self.weights, x) + self.bias\n",
        "            \n",
        "            def updateWeights(self, update):\n",
        "                print(\"Before:\", self.weights)\n",
        "                self.weights -= update\n",
        "                print(\"After:\", self.weights)\n",
        "            \n",
        "        \n",
        "        def __init__(self, layerType, neuronsPerLayer, inputShape):\n",
        "            self.layerType = layerType\n",
        "            self.numNeurons = neuronsPerLayer\n",
        "            self.inputShape = inputShape\n",
        "            self.neurons = [self.Neuron(inputShape) for _ in range(neuronsPerLayer)]\n",
        "            self.dWeightedSum_dWeights = []\n",
        "            self.dWeightedSum_dInput = []\n",
        "            \n",
        "            if layerType == \"Output\":\n",
        "                self.dSoftmax = []\n",
        "            elif layerType == \"Dense\":\n",
        "                self.dRelu = []\n",
        "            \n",
        "        def getInputShape(self):\n",
        "            return self.inputShape\n",
        "        \n",
        "        def getNumNeurons(self):\n",
        "            return self.numNeurons\n",
        "        \n",
        "        def getDSoftmax(self):\n",
        "            temp = self.dSoftmax\n",
        "            self.dSoftmax = []\n",
        "            return temp\n",
        "        \n",
        "        def getDWeightedSum_dWeights(self):\n",
        "            temp = self.dWeightedSum_dWeights\n",
        "            self.dWeightedSum_dWeights = []\n",
        "            return temp\n",
        "        \n",
        "        def getDWeightedSum_dInput(self):\n",
        "            temp = self.dWeightedSum_dInput\n",
        "            self.dWeightedSum_dInput = []\n",
        "            return temp\n",
        "        \n",
        "        def getDRelu(self):\n",
        "            temp = self.dRelu\n",
        "            self.dRelu = []\n",
        "            return temp\n",
        "        \n",
        "        def generateLayerOutput(self, x):\n",
        "            layerOutput = np.array([])\n",
        "            change = [self.calcDWeightedSum_dWeights(x) for _ in range(self.numNeurons)]\n",
        "            self.dWeightedSum_dWeights.append(change)\n",
        "            \n",
        "            change = []\n",
        "            \n",
        "            for neuron in self.neurons:\n",
        "                change.append(neuron.getWeights())\n",
        "                neuronOutput = neuron.generateNeuronOutput(x)\n",
        "                layerOutput = np.append(layerOutput, neuronOutput)\n",
        "                \n",
        "            self.dWeightedSum_dInput.append(change)\n",
        "               \n",
        "            print(self.layerType, \":\", layerOutput)\n",
        "            if self.layerType == \"Dense\":\n",
        "                #Leaky ReLU activation\n",
        "                change = self.calcDRelu(layerOutput)\n",
        "                self.dRelu.append(change)\n",
        "                layerOutput[layerOutput < 0] *= 0.01\n",
        "            elif self.layerType == \"Output\":\n",
        "                if len(layerOutput) == 1:\n",
        "                    #Sigmoid activation\n",
        "                    layerOutput = 1 / (1 + math.exp(-1*layerOutput[0]))\n",
        "                    #print(\"Probabilities: \", layerOutput)\n",
        "                else:\n",
        "                    #Softmax activation\n",
        "                    layerOutput = np.exp(layerOutput)/sum(np.exp(layerOutput))\n",
        "                    change = self.calcDSoftmax(layerOutput)\n",
        "                    self.dSoftmax.append(change)\n",
        "                    #print(self.dSoftmax)\n",
        "                    #print(\"Probabilities: \", layerOutput)\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "                    \n",
        "            print(\"After activation - \", layerOutput)\n",
        "            return layerOutput\n",
        "        \n",
        "        # Derivative of softmax with respect to weighted sum/dot product\n",
        "        def calcDSoftmax(self, prob):\n",
        "            n = len(prob)\n",
        "            return [prob[i]-(prob[i]**2) for i in range(n)]\n",
        "        \n",
        "        # Derivative of weighted sum with respect to the weights\n",
        "        # This function is kinda pointless as it just returns the argument\n",
        "        # passed to it unaltered but I added it in to remind me that this\n",
        "        # is the derivative.\n",
        "        def calcDWeightedSum_dWeights(self, x):\n",
        "            return x\n",
        "        \n",
        "        # Derivative of weighted sum with respect to the input provided\n",
        "        # Also a redundant function, and was also added in for the same\n",
        "        # reason as above\n",
        "        def calcDWeightedSum_dInput(self, weights):\n",
        "            return weights\n",
        "        \n",
        "        # Derivative of Leaky ReLU with respect to weighted sum/dot product\n",
        "        def calcDRelu(self, input_vector):\n",
        "            derivative = np.array(input_vector)\n",
        "            derivative[derivative > 0] = 1\n",
        "            derivative[derivative <= 0] = 0.01\n",
        "            return derivative\n",
        "        \n",
        "        def updateWeights(self, update):\n",
        "            for i in range(self.numNeurons):\n",
        "                self.neurons[i].updateWeights(update[i])\n",
        "            \n",
        "        def __str__(self):\n",
        "            output = \"\"\n",
        "            for neuron in self.neurons:\n",
        "                weights, bias = neuron.getWeightsAndBias()\n",
        "                output+=\"\\t\"\n",
        "                \n",
        "                for j in range(0, len(weights)):\n",
        "                    output+=(\"w{}: {}, \".format(j, weights[j]))\n",
        "                    \n",
        "                output+=(\"b0: {}\\n\".format(bias))\n",
        "            \n",
        "            return output\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    \n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.numLayers = 0\n",
        "        \n",
        "    def addLayer(self, layerType, neuronsPerLayer, inputShape=None):\n",
        "        if inputShape == None:\n",
        "            inputShape = self.layers[-1].getNumNeurons()\n",
        "            \n",
        "        self.layers.append(self.Layer(layerType, neuronsPerLayer, inputShape))\n",
        "        self.numLayers+=1\n",
        "        \n",
        "    def predict(self, x, labels, batch_size=2, step_size = 0.1):\n",
        "        probabilities = []\n",
        "        dError = []\n",
        "        counter = 0\n",
        "        \n",
        "        for i in range(len(x)):\n",
        "            prob = self.feedForward(x[i])\n",
        "            change = self.calcDError(prob, labels[i])\n",
        "            dError.append(change)\n",
        "            counter+=1\n",
        "            \n",
        "            if counter == batch_size:\n",
        "                counter = 0\n",
        "                self.backPropagation(dError, step_size, batch_size)\n",
        "                dError = []\n",
        "                \n",
        "            #print(pred)\n",
        "            probabilities.append(prob)\n",
        "      \n",
        "        predictions = np.argmax(probabilities, axis=1)\n",
        "        #loss = self.calcTotalLoss(probabilities, labels)\n",
        "        #print(loss)\n",
        "        return predictions\n",
        "    \n",
        "    def feedForward(self, x):\n",
        "        lastLayerOutput = x\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            lastLayerOutput = layer.generateLayerOutput(lastLayerOutput)\n",
        "            \n",
        "        return lastLayerOutput\n",
        "    \n",
        "    def backPropagation(self, dError, step_size, batch_size=None):\n",
        "        #Only updates output layer weights for now\n",
        "        d1 = np.array(dError)\n",
        "        d2 = np.array(self.layers[-1].getDSoftmax())\n",
        "        d3 = np.array(self.layers[-1].getDWeightedSum_dWeights())\n",
        "        print(\"d1 before:\", d1)\n",
        "        print(\"d2 before:\", d2)\n",
        "        print(\"d3 before:\", d3[0])\n",
        "        shape = np.shape(d3[0])\n",
        "        print(shape)\n",
        "        onesArray = np.ones((shape[1], shape[0]))\n",
        "        d1 = [np.transpose(d1[i]*onesArray) for i in range(batch_size)]\n",
        "        d2 = [np.transpose(d2[i]*onesArray) for i in range(batch_size)]\n",
        "        print(\"d1:\", d1)\n",
        "        print(\"d2:\", d2)\n",
        "        print(\"d3:\", d3[0])\n",
        "        changeOutputWeights = [d1[i]*d2[i]*d3[i] for i in range(len(d3))]\n",
        "        avgChange = (sum(changeOutputWeights)/batch_size) * step_size\n",
        "        #print(\"change:\", changeOutputWeights)\n",
        "        #print(\"fin change:\", avgChange)\n",
        "        self.layers[-1].updateWeights(avgChange)\n",
        "        \n",
        "        # Derivatives for H[-1] \n",
        "        d4 = np.array(self.layers[-1].getDWeightedSum_dInput())\n",
        "        d5 = np.array(self.layers[-2].getDRelu())\n",
        "        d6 = np.array(self.layers[-2].getDWeightedSum_dWeights())\n",
        "        print(\"d4:\", d4)\n",
        "        print(\"d6:\", d6)\n",
        "        dTotalError = np.array([np.sum(d1[i]*d2[i]*d4[i], axis=1) for i in range(batch_size)])\n",
        "        #numWeights = len(dTotalError[0, 0])\n",
        "        print(\"dTotal before:\",dTotalError)\n",
        "        numNeurons = len(dTotalError[0])\n",
        "        shape = np.shape(d6[0])\n",
        "        print(shape)\n",
        "        onesArray = np.ones((shape[1], shape[0]))\n",
        "        print(onesArray)\n",
        "        d5 =[np.transpose(d5[i]*onesArray) for i in range(batch_size)]\n",
        "        print(\"d5:\", d5)\n",
        "        #dTotalError = [np.transpose(dTotalError[i]*onesArray) for i in range(batch_size)]\n",
        "        dTotalError = [[[dTotalError[j, i] for _ in range(shape[1])] for i in range(len(dTotalError[0]))] for j in range(batch_size)]\n",
        "        print(\"dTotalError sum:\", dTotalError)\n",
        "        changeHiddenWeights = [d5[i]*d6[i]*dTotalError[i] for i in range(batch_size)]\n",
        "        avgChange = (sum(changeHiddenWeights)/batch_size) * step_size\n",
        "        \n",
        "        \n",
        "        print(\"Change:\", changeHiddenWeights)\n",
        "        print(\"Avg:\", avgChange)\n",
        "        \n",
        "        self.layers[-2].updateWeights(avgChange)\n",
        "        # End of derivatives for H[-1] \n",
        "        \n",
        "        # Start of loop experiments\n",
        "        #cumulativeDTotalError = [[d2[i] for _ in range()]]\n",
        "        \n",
        "        #for i in range(-2, -1*(self.numLayers+1), -1):\n",
        "            #d4 = np.array(self.layers[-1].getDWeightedSum_dInput())\n",
        "            \n",
        "        # End of loop experiments\n",
        "    \n",
        "    #Derivative of error (cross-entropy) with respect to softmax probabilities\n",
        "    def calcDError(self, prob, actual):\n",
        "        n = len(actual)\n",
        "        return [-1*(actual[i]/prob[i]) + ((1 - actual[i])*(-1/(1-prob[i]))) for i in range(n)]\n",
        "    \n",
        "    def calcTotalLoss(self, prob, actual):\n",
        "        #REDO THIS TO WORK WITH ONE HOT ENCODING\n",
        "        n = len(actual)\n",
        "        loss = -1*sum(np.log([prob[i][actual[i]] for i in range(n)]))/n\n",
        "        return loss\n",
        "        \n",
        "    def __str__(self):\n",
        "        output = \"\"\n",
        "        \n",
        "        for i in range(0, self.numLayers):\n",
        "            output+=(\"Layer {}:\\n\".format(i+1))\n",
        "            output+=str(self.layers[i])\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMEeIQ7_whBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn = NeuralNetwork()\n",
        "nn.addLayer(\"Dense\", 3, inputShape=4)\n",
        "nn.addLayer(\"Dense\", 4)\n",
        "nn.addLayer(\"Dense\", 4)\n",
        "nn.addLayer(\"Output\", 3)\n",
        "#print(nn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIUioca4zaw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris_X, iris_y = load_iris(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBeLqlxqEhRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "iris_X_trimmed = iris_X\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "iris_X_scaled = scaler.fit_transform(iris_X_trimmed)\n",
        "iris_X_zscore = stats.zscore(iris_X_trimmed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E43ANN9uzaQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(iris_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjMHZ5k0GRM9",
        "colab_type": "code",
        "outputId": "f22769ad-ad7a-4588-c761-0aef51ce6486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pred = nn.predict(iris_X_zscore[0:2, :], labels[0:2])\n",
        "accuracy_score(pred, iris_y[0:2])"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dense : [-0.36521343 -0.15329216  1.33409062]\n",
            "After activation -  [-0.00365213 -0.00153292  1.33409062]\n",
            "Dense : [-1.27026107  1.4137485  -1.03275754  1.7570335 ]\n",
            "After activation -  [-0.01270261  1.4137485  -0.01032758  1.7570335 ]\n",
            "Dense : [-1.17821565  0.69614572  1.36799947  0.72791376]\n",
            "After activation -  [-0.01178216  0.69614572  1.36799947  0.72791376]\n",
            "Output : [-0.58081774  0.73168135  2.29492744]\n",
            "After activation -  [0.04453533 0.16546866 0.78999601]\n",
            "Dense : [-0.55830914  0.09344257  2.26455591]\n",
            "After activation -  [-0.00558309  0.09344257  2.26455591]\n",
            "Dense : [-1.574991    2.09982925 -1.18145366  2.29197975]\n",
            "After activation -  [-0.01574991  2.09982925 -0.01181454  2.29197975]\n",
            "Dense : [-1.16445075  1.48678599  1.83895589  1.47648521]\n",
            "After activation -  [-0.01164451  1.48678599  1.83895589  1.47648521]\n",
            "Output : [-0.90337939  1.24645729  3.14042641]\n",
            "After activation -  [0.01500905 0.1288295  0.85616145]\n",
            "d1 before: [[-22.45408498  -1.19827735  -4.76181439]\n",
            " [-66.62648118  -1.14788093  -6.95223915]]\n",
            "d2 before: [[0.04255193 0.13808878 0.16590231]\n",
            " [0.01478378 0.11223246 0.12314902]]\n",
            "d3 before: [[-0.01178216  0.69614572  1.36799947  0.72791376]\n",
            " [-0.01178216  0.69614572  1.36799947  0.72791376]\n",
            " [-0.01178216  0.69614572  1.36799947  0.72791376]]\n",
            "(3, 4)\n",
            "d1: [array([[-22.45408498, -22.45408498, -22.45408498, -22.45408498],\n",
            "       [ -1.19827735,  -1.19827735,  -1.19827735,  -1.19827735],\n",
            "       [ -4.76181439,  -4.76181439,  -4.76181439,  -4.76181439]]), array([[-66.62648118, -66.62648118, -66.62648118, -66.62648118],\n",
            "       [ -1.14788093,  -1.14788093,  -1.14788093,  -1.14788093],\n",
            "       [ -6.95223915,  -6.95223915,  -6.95223915,  -6.95223915]])]\n",
            "d2: [array([[0.04255193, 0.04255193, 0.04255193, 0.04255193],\n",
            "       [0.13808878, 0.13808878, 0.13808878, 0.13808878],\n",
            "       [0.16590231, 0.16590231, 0.16590231, 0.16590231]]), array([[0.01478378, 0.01478378, 0.01478378, 0.01478378],\n",
            "       [0.11223246, 0.11223246, 0.11223246, 0.11223246],\n",
            "       [0.12314902, 0.12314902, 0.12314902, 0.12314902]])]\n",
            "d3: [[-0.01178216  0.69614572  1.36799947  0.72791376]\n",
            " [-0.01178216  0.69614572  1.36799947  0.72791376]\n",
            " [-0.01178216  0.69614572  1.36799947  0.72791376]]\n",
            "Before: [-0.50398567 -0.0071138  -0.33602495 -0.21188988]\n",
            "After: [-0.50512203  0.09936686 -0.18010344 -0.10439886]\n",
            "Before: [-1.13241206  0.08968975  0.56052108  0.24050942]\n",
            "After: [-1.13258455  0.10502636  0.58368472  0.25604251]\n",
            "Before: [0.06860885 0.57152566 0.15817176 0.42631376]\n",
            "After: [0.06764498 0.66266972 0.29092962 0.5182717 ]\n",
            "Num weights: 4\n",
            "d4: [[[-0.50512203  0.09936686 -0.18010344 -0.10439886]\n",
            "  [-1.13258455  0.10502636  0.58368472  0.25604251]\n",
            "  [ 0.06764498  0.66266972  0.29092962  0.5182717 ]]\n",
            "\n",
            " [[-0.50512203  0.09936686 -0.18010344 -0.10439886]\n",
            "  [-1.13258455  0.10502636  0.58368472  0.25604251]\n",
            "  [ 0.06764498  0.66266972  0.29092962  0.5182717 ]]]\n",
            "d6: [[[-0.01270261  1.4137485  -0.01032758  1.7570335 ]\n",
            "  [-0.01270261  1.4137485  -0.01032758  1.7570335 ]\n",
            "  [-0.01270261  1.4137485  -0.01032758  1.7570335 ]\n",
            "  [-0.01270261  1.4137485  -0.01032758  1.7570335 ]]\n",
            "\n",
            " [[-0.01574991  2.09982925 -0.01181454  2.29197975]\n",
            "  [-0.01574991  2.09982925 -0.01181454  2.29197975]\n",
            "  [-0.01574991  2.09982925 -0.01181454  2.29197975]\n",
            "  [-0.01574991  2.09982925 -0.01181454  2.29197975]]]\n",
            "dTotal before: [[ 0.65951663  0.03108014 -1.21621152]\n",
            " [ 0.67989736  0.02419817 -1.31807427]]\n",
            "(4, 4)\n",
            "[[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n",
            "d5: [array([[0.01, 0.01, 0.01, 0.01],\n",
            "       [1.  , 1.  , 1.  , 1.  ],\n",
            "       [1.  , 1.  , 1.  , 1.  ],\n",
            "       [1.  , 1.  , 1.  , 1.  ]]), array([[0.01, 0.01, 0.01, 0.01],\n",
            "       [1.  , 1.  , 1.  , 1.  ],\n",
            "       [1.  , 1.  , 1.  , 1.  ],\n",
            "       [1.  , 1.  , 1.  , 1.  ]])]\n",
            "dTotalError sum: [[[0.659516628231265, 0.659516628231265, 0.659516628231265, 0.659516628231265], [0.031080137440140605, 0.031080137440140605, 0.031080137440140605, 0.031080137440140605], [-1.2162115193101666, -1.2162115193101666, -1.2162115193101666, -1.2162115193101666]], [[0.6798973628973355, 0.6798973628973355, 0.6798973628973355, 0.6798973628973355], [0.024198169444159173, 0.024198169444159173, 0.024198169444159173, 0.024198169444159173], [-1.3180742672122576, -1.3180742672122576, -1.3180742672122576, -1.3180742672122576]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-207-41c7052c7e42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris_X_zscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miris_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-205-f066a5a6893b>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, labels, batch_size, step_size)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m                 \u001b[0mdError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-205-f066a5a6893b>\u001b[0m in \u001b[0;36mbackPropagation\u001b[0;34m(self, dError, step_size, batch_size)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mdTotalError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdTotalError\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdTotalError\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dTotalError sum:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdTotalError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mchangeHiddenWeights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md6\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdTotalError\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mavgChange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchangeHiddenWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-205-f066a5a6893b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mdTotalError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdTotalError\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdTotalError\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dTotalError sum:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdTotalError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mchangeHiddenWeights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md6\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdTotalError\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mavgChange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchangeHiddenWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,4) (3,4) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N59zxfHfOMhw",
        "colab_type": "code",
        "outputId": "d93d1cb7-2aa3-4388-d7e3-45c231153fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print(iris_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt5W16nnS-KG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "da4466a4-c882-4c3a-c982-6ab364f398f9"
      },
      "source": [
        "x = [[1, 2, 3], [4, 5, 6]]\n",
        "\n",
        "y = [[[x[j][i] for _ in range(4)] for i in range(len(x[0]))] for j in range(len(x))]\n",
        "y"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]],\n",
              " [[4, 4, 4, 4], [5, 5, 5, 5], [6, 6, 6, 6]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_VpcT4Foxmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}