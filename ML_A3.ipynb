{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML A3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/13194307/UTS_ML2019_ID13194307/blob/master/ML_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ6JcIYRxeCK",
        "colab_type": "text"
      },
      "source": [
        "# Question\n",
        "**One of the themes in the machine learning models we've looked at this semester is\n",
        "large numbers of parameters that are changed by tiny amounts. Why do so many\n",
        "apparently different models use such similar techniques? Are there other ways to\n",
        "approach the problem of learning? Are there also commonalities in the way the\n",
        "amounts to be changed are determined? (800-1000 words)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puNObDNCxmc0",
        "colab_type": "text"
      },
      "source": [
        "**Why do so many apparently different models use such similar techniques?**\n",
        "\n",
        "The reason as to why so many models learn by making small changes to a large number of parameters is due to the nature of machine learning problems themselves. Most models operate under a fundamental assumption that for any machine learning problem, there exists some function that governs the distribution of the data points in some way. This assumption is neither baseless nor naive and mathematical functions form the backbone of many things in this world, a point thoroughly discussed in an essay by Ransford [1]. Therefore the goal of machine learning can be reframed as trying to find a function that best approximates the actual function for the distribution of the data, with each trainable parameter representing a variable of the function. The vast majority of machine learning problems are non-trivial and in order to generate a function that capture the intricacies of a complex problem, a large number of parameters are needed. This is why most models utilise a large number of parameters. \n",
        "\n",
        "Realistically, no model will be able to uncover the actual distribution of the dataset as such a task would not only require a currently-unachievable amount of processing power and memory but also a huge sample size. As a result, models treat the task of finding the ideal function as an optimisation problem where some loss or cost function is minimised. One question remains however; given a dataset following the rules of some unknown function, how do you find the optimal parameter configuration that provides minimal loss? One way would be to greedily try every possible parameter configuration, but this would be computationally intractable. The domain of this greedy search could be constrained. However there would need to be some way of determining the appropriate constraints and even with constraints, this operation would also most likely be intractable. A more reasonable alternative would be to, given some starting configuration, change the value of the weights in some direction that will reduce the loss. Small steps, rather than large steps, should be taken as although moving in some direction might reduce loss in the short term, the complex nature of most data distributions makes it unlikely that constantly moving in this same direction will continue to yield parameter configurations that produce less loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-lBQbBOEfOb",
        "colab_type": "text"
      },
      "source": [
        "**Are there other ways to approach the problem of learning?**\n",
        "\n",
        "The typical approach of making small changes to a large number of parameters works well for many models. However there are other ways of approaching the problem of machine learning. Some of these approaches are only theoretical while others are used in practise for machine learning.\n",
        "\n",
        "One approach, which was mentioned previously, is a greedy search for the parameter configuration that produces the least amount of loss. This is a purely theoretical method of machine learning and has many problems associated with it were it to be implemented. Firstly, without some constraints on the search domain, this search would be impossible to perform. As the constraints get stricter, the chances of fin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjUoqY-JEiHF",
        "colab_type": "text"
      },
      "source": [
        "**Are there also commonalities in the way the amounts to be changed are determined?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueTyo5z-E1FE",
        "colab_type": "text"
      },
      "source": [
        "# References"
      ]
    }
  ]
}