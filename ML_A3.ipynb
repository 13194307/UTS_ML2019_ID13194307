{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML A3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/13194307/UTS_ML2019_ID13194307/blob/master/ML_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ6JcIYRxeCK",
        "colab_type": "text"
      },
      "source": [
        "# Question\n",
        "**One of the themes in the machine learning models we've looked at this semester is\n",
        "large numbers of parameters that are changed by tiny amounts. Why do so many\n",
        "apparently different models use such similar techniques? Are there other ways to\n",
        "approach the problem of learning? Are there also commonalities in the way the\n",
        "amounts to be changed are determined? (800-1000 words)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puNObDNCxmc0",
        "colab_type": "text"
      },
      "source": [
        "**Why do so many apparently different models use such similar techniques?**\n",
        "\n",
        "The reason as to why so many models learn by making small changes to a large number of parameters is due to the nature of machine learning problems themselves. Most models operate under a fundamental assumption that for any machine learning problem, there exists some function that governs the distribution of the data points. This assumption is neither baseless nor naive and mathematical functions form the backbone of many things in this world, a point thoroughly discussed in an essay by Ransford [1]. Therefore the goal of machine learning can be reframed as trying to find a function that best approximates the actual function for the distribution of the data, with each trainable parameter representing a variable of the function. The vast majority of machine learning problems are non-trivial and in order to generate a function that capture the intricacies of a complex problem, a large number of parameters are needed. This is why most models utilise a large number of parameters. \n",
        "\n",
        "Realistically, no model will be able to uncover the actual distribution of the dataset as such a task would not only require a currently-unachievable amount of processing power and memory but also a huge sample size. As a result, models treat the task of finding the ideal function as an optimisation problem where some loss or cost function is minimised. One question remains however; given a dataset following the rules of some unknown function, how do you find the optimal parameter configuration that provides minimal loss? One way would be to greedily try every possible parameter configuration, but this would be computationally intractable. The domain of this greedy search could be constrained. However there would need to be some way of determining the appropriate constraints and even with constraints, this operation would also most likely be intractable. A more reasonable alternative would be to, given some starting configuration, change the value of the weights in some direction that will reduce the loss. Small steps, rather than large steps, should be taken as although moving in some direction might reduce loss in the short term, the complex nature of most data distributions makes it unlikely that constantly moving in this same direction will continue to yield parameter configurations that produce less loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-lBQbBOEfOb",
        "colab_type": "text"
      },
      "source": [
        "**Are there other ways to approach the problem of learning?**\n",
        "\n",
        "The typical approach of making small changes to a large number of parameters works well for many models. However there are other ways of approaching the problem of machine learning. Some of these approaches are only theoretical while others are used in practise for machine learning.\n",
        "\n",
        "One approach, which was mentioned previously, is a greedy search for the parameter configuration that produces the least amount of loss. This is a purely theoretical method of machine learning and has many problems associated with it, were it to be implemented. Firstly, without some constraints on the search domain, this search would be impossible to perform. As the constraints get stricter, the search time decreases but the chance of finding the optimal parameter configuration decreases as well. Determining the constraints would also be problematic as there isn't much that could be used to discover the ideal domain constrains. A big advantage of this method is that, unlike the typical gradient descent approaches, a greedy approach would not get stuck in local minimas. However this advantage, for now, is not enough to offset the amount of computational power required.\n",
        "\n",
        "Many models choose to either have only untrainable parameters or no parameters at all. The K-Nearest Neighbours (KNN) algorithm is an example of such a model, with its only parameter being the variable K, which determines the number of neighbours to be considered when making predictions. Instead of updating weights or parameters during training, the KNN algorithm simply stores the value of the training data points. The class of an input data point is decided through a vote from the K nearest data points processed during training. It is possible to optimise the value of K, therefore indirectly turning K into a trainable parameter. However, as of now, there is no way to link the value of K to the loss function and therefore no way to determine the direction to shift the value of K. Instead, the optimisation process would require a greedy approach where various values of K are tested. A Naive Bayes Classifier has no parameters at all, and predictions are made through applying Bayes Theorem under the assumption that the input attributes are independent of each other. Due to the fact tht there are no parameters, no optimisation can be conducted on a Naive Bayes Classifier. Due to the limited number or lack of parameters in these models, they tend to not perform as well as other models which can more easily capture the complex nature of machine learning problems.\n",
        "\n",
        "Other models have parameters that define a function, but the value of these parameters are determined through other means. A Support Vector Machine (SVM) is a model which attempts to find the best hyperplane that linearly separates the data. While a soft margin SVM has a cost function that needs to be minimised, a hard margin SVM assumes that the dataset is linearly separable when the data is either kept in its original dimensions or is converted to a higher dimensional representation. Therefore, no cost function is needed. With this assumption, the best fit hyperplane is defined as the one that maximises the margins between the support vectors. The ideal parameters for the separating hyperplane is determined through application of linear algebra techniques, such as analysis of the saddle points of Lagrangians [2]. This approach sidesteps the need to constantly make small changes to parameter and despite their lack of trainable parameters, SVMs are well known for achieving high levels of performance on many classification problems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjUoqY-JEiHF",
        "colab_type": "text"
      },
      "source": [
        "**Are there also commonalities in the way the amounts to be changed are determined?**\n",
        "\n",
        "Most models utilise gradient descent to dictate how parameters are changed. Gradient descent involves calculating the derivative for each parameter with respect to some loss or cost function. This will give the gradient, with the polarity of the gradient representing the direction where loss will increase. Therefore to generate a decrease in loss, the values of the parameters should be shifted in the direction opposite to that of the gradient. The magnitude of the change in parameters is simply the gradient multiplied by some predetermined step size. Neural networks are well known for making use of gradient descent, with it forming the backbone of the famous backpropagation algorithm[3]. There are many ways in which the implementation of a gradient descent optimisation algorithm can vary. For example, stochastic gradient descent calculates the gradients with a batch size of 1 while mini-batch gradient descent uses a custom batch size. However, they all calculate the gradients in the same way and use the magnitude and direction of the gradients to calculate how much each parameter should be changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueTyo5z-E1FE",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "https://www.sciencedirect.com/science/article/pii/S1877050915035759"
      ]
    }
  ]
}