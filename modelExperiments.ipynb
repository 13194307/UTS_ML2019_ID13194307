{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/13194307/UTS_ML2019_ID13194307/blob/master/modelExperiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJMPAeH_opWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "from google.colab import files\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, log_loss\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR6Xm7d4o9We",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports dataset from github repo\n",
        "\n",
        "urlDataset = r\"https://raw.githubusercontent.com/13194307/UTS_ML2019_ID13194307/master/ML_A2_PracticalProject/modifiedDataset.csv\"\n",
        "urlTest = r\"https://raw.githubusercontent.com/13194307/UTS_ML2019_ID13194307/master/ML_A2_PracticalProject/test.csv\"\n",
        "dataset = pd.read_csv(urlDataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmcKKxqFJgdZ",
        "colab_type": "code",
        "outputId": "81d7dbaa-101b-4084-8868-5dd7368f18ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>AnimalID</th>\n",
              "      <th>Name</th>\n",
              "      <th>DateTime</th>\n",
              "      <th>OutcomeType</th>\n",
              "      <th>AnimalType</th>\n",
              "      <th>SexuponOutcome</th>\n",
              "      <th>AgeuponOutcome</th>\n",
              "      <th>Breed</th>\n",
              "      <th>Color</th>\n",
              "      <th>AgeInDays</th>\n",
              "      <th>Gender</th>\n",
              "      <th>IsDesexed</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Month</th>\n",
              "      <th>Year</th>\n",
              "      <th>IsWeekend</th>\n",
              "      <th>Hour</th>\n",
              "      <th>TimeOfDay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A671945</td>\n",
              "      <td>Hambone</td>\n",
              "      <td>12-02-14 18:22</td>\n",
              "      <td>Return</td>\n",
              "      <td>Dog</td>\n",
              "      <td>Neutered Male</td>\n",
              "      <td>1 year</td>\n",
              "      <td>Shetland Sheepdog</td>\n",
              "      <td>Brown</td>\n",
              "      <td>365</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>Feb</td>\n",
              "      <td>14</td>\n",
              "      <td>False</td>\n",
              "      <td>18</td>\n",
              "      <td>Evening</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A656520</td>\n",
              "      <td>Emily</td>\n",
              "      <td>13-10-13 12:44</td>\n",
              "      <td>Euthanasia</td>\n",
              "      <td>Cat</td>\n",
              "      <td>Spayed Female</td>\n",
              "      <td>1 year</td>\n",
              "      <td>Domestic Shorthair</td>\n",
              "      <td>Cream Tabby</td>\n",
              "      <td>365</td>\n",
              "      <td>Female</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Sunday</td>\n",
              "      <td>Oct</td>\n",
              "      <td>13</td>\n",
              "      <td>True</td>\n",
              "      <td>13</td>\n",
              "      <td>Afternoon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A686464</td>\n",
              "      <td>Pearce</td>\n",
              "      <td>31-01-15 12:28</td>\n",
              "      <td>Adoption</td>\n",
              "      <td>Dog</td>\n",
              "      <td>Neutered Male</td>\n",
              "      <td>2 years</td>\n",
              "      <td>Pit Bull</td>\n",
              "      <td>Blue</td>\n",
              "      <td>730</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Saturday</td>\n",
              "      <td>Jan</td>\n",
              "      <td>15</td>\n",
              "      <td>True</td>\n",
              "      <td>12</td>\n",
              "      <td>Morning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>A683430</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>11-07-14 19:09</td>\n",
              "      <td>Transfer</td>\n",
              "      <td>Cat</td>\n",
              "      <td>Intact Male</td>\n",
              "      <td>3 weeks</td>\n",
              "      <td>Domestic Shorthair</td>\n",
              "      <td>Blue Cream</td>\n",
              "      <td>21</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Friday</td>\n",
              "      <td>Jul</td>\n",
              "      <td>14</td>\n",
              "      <td>False</td>\n",
              "      <td>19</td>\n",
              "      <td>Evening</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>A667013</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>15-11-13 12:52</td>\n",
              "      <td>Transfer</td>\n",
              "      <td>Dog</td>\n",
              "      <td>Neutered Male</td>\n",
              "      <td>2 years</td>\n",
              "      <td>Lhasa Apso</td>\n",
              "      <td>Tan</td>\n",
              "      <td>730</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Friday</td>\n",
              "      <td>Nov</td>\n",
              "      <td>13</td>\n",
              "      <td>False</td>\n",
              "      <td>13</td>\n",
              "      <td>Afternoon</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0 AnimalID     Name  ... IsWeekend Hour  TimeOfDay\n",
              "0           0  A671945  Hambone  ...     False   18    Evening\n",
              "1           1  A656520    Emily  ...      True   13  Afternoon\n",
              "2           2  A686464   Pearce  ...      True   12    Morning\n",
              "3           3  A683430  Unknown  ...     False   19    Evening\n",
              "4           4  A667013  Unknown  ...     False   13  Afternoon\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQdRX6BPpCY0",
        "colab_type": "code",
        "outputId": "52f5f8b7-2735-4b0b-9197-db20b371c7b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "x = dataset.drop([\"Unnamed: 0\", \"AnimalID\", \"Name\", \"DateTime\",\n",
        "                  \"OutcomeType\", \"Breed\", \"SexuponOutcome\", \"AgeuponOutcome\", \"Year\"], axis=1)\n",
        "x = pd.get_dummies(x)\n",
        "y = dataset[\"OutcomeType\"]\n",
        "\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "x = pd.DataFrame(scaler.fit_transform(x))\n",
        "\n",
        "x.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.395879</td>\n",
              "      <td>-0.692592</td>\n",
              "      <td>0.907842</td>\n",
              "      <td>-0.844954</td>\n",
              "      <td>0.844954</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>-0.031204</td>\n",
              "      <td>-0.562357</td>\n",
              "      <td>-0.060972</td>\n",
              "      <td>-0.048218</td>\n",
              "      <td>-0.047827</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>-0.216713</td>\n",
              "      <td>-0.026671</td>\n",
              "      <td>-0.078813</td>\n",
              "      <td>-0.034075</td>\n",
              "      <td>-0.013678</td>\n",
              "      <td>-0.161325</td>\n",
              "      <td>-0.040606</td>\n",
              "      <td>-0.017303</td>\n",
              "      <td>3.563728</td>\n",
              "      <td>-0.163871</td>\n",
              "      <td>-0.051971</td>\n",
              "      <td>-0.3277</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>-0.100449</td>\n",
              "      <td>-0.145214</td>\n",
              "      <td>-0.031799</td>\n",
              "      <td>-0.130562</td>\n",
              "      <td>-0.028041</td>\n",
              "      <td>-0.091308</td>\n",
              "      <td>-0.102332</td>\n",
              "      <td>-0.088774</td>\n",
              "      <td>-0.056482</td>\n",
              "      <td>-0.05375</td>\n",
              "      <td>-0.092549</td>\n",
              "      <td>-0.043723</td>\n",
              "      <td>-0.038226</td>\n",
              "      <td>-0.023696</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044574</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>-0.039195</td>\n",
              "      <td>-0.258482</td>\n",
              "      <td>-0.122944</td>\n",
              "      <td>-0.148931</td>\n",
              "      <td>-0.035688</td>\n",
              "      <td>-0.175652</td>\n",
              "      <td>-0.37815</td>\n",
              "      <td>-0.093164</td>\n",
              "      <td>-0.034075</td>\n",
              "      <td>-0.925440</td>\n",
              "      <td>1.004537</td>\n",
              "      <td>-0.206582</td>\n",
              "      <td>-0.597733</td>\n",
              "      <td>-0.206582</td>\n",
              "      <td>0.661151</td>\n",
              "      <td>-0.393636</td>\n",
              "      <td>-0.400581</td>\n",
              "      <td>-0.440763</td>\n",
              "      <td>-0.438885</td>\n",
              "      <td>-0.380216</td>\n",
              "      <td>-0.413074</td>\n",
              "      <td>2.571984</td>\n",
              "      <td>-0.259715</td>\n",
              "      <td>-0.297401</td>\n",
              "      <td>-0.334586</td>\n",
              "      <td>3.642896</td>\n",
              "      <td>-0.309314</td>\n",
              "      <td>-0.321645</td>\n",
              "      <td>-0.308224</td>\n",
              "      <td>-0.243662</td>\n",
              "      <td>-0.291549</td>\n",
              "      <td>-0.332994</td>\n",
              "      <td>-0.347573</td>\n",
              "      <td>-0.284696</td>\n",
              "      <td>-0.800829</td>\n",
              "      <td>1.251163</td>\n",
              "      <td>-0.504626</td>\n",
              "      <td>-0.129521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.395879</td>\n",
              "      <td>1.443852</td>\n",
              "      <td>-0.557213</td>\n",
              "      <td>1.183497</td>\n",
              "      <td>-1.183497</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>-0.031204</td>\n",
              "      <td>-0.562357</td>\n",
              "      <td>-0.060972</td>\n",
              "      <td>-0.048218</td>\n",
              "      <td>-0.047827</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>-0.216713</td>\n",
              "      <td>-0.026671</td>\n",
              "      <td>-0.078813</td>\n",
              "      <td>-0.034075</td>\n",
              "      <td>-0.013678</td>\n",
              "      <td>-0.161325</td>\n",
              "      <td>-0.040606</td>\n",
              "      <td>-0.017303</td>\n",
              "      <td>-0.280605</td>\n",
              "      <td>-0.163871</td>\n",
              "      <td>-0.051971</td>\n",
              "      <td>-0.3277</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>-0.100449</td>\n",
              "      <td>-0.145214</td>\n",
              "      <td>-0.031799</td>\n",
              "      <td>-0.130562</td>\n",
              "      <td>-0.028041</td>\n",
              "      <td>-0.091308</td>\n",
              "      <td>9.772133</td>\n",
              "      <td>-0.088774</td>\n",
              "      <td>-0.056482</td>\n",
              "      <td>-0.05375</td>\n",
              "      <td>-0.092549</td>\n",
              "      <td>-0.043723</td>\n",
              "      <td>-0.038226</td>\n",
              "      <td>-0.023696</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044574</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>-0.039195</td>\n",
              "      <td>-0.258482</td>\n",
              "      <td>-0.122944</td>\n",
              "      <td>-0.148931</td>\n",
              "      <td>-0.035688</td>\n",
              "      <td>-0.175652</td>\n",
              "      <td>-0.37815</td>\n",
              "      <td>-0.093164</td>\n",
              "      <td>-0.034075</td>\n",
              "      <td>1.080568</td>\n",
              "      <td>-0.995483</td>\n",
              "      <td>-0.206582</td>\n",
              "      <td>-0.597733</td>\n",
              "      <td>-0.206582</td>\n",
              "      <td>0.661151</td>\n",
              "      <td>-0.393636</td>\n",
              "      <td>-0.400581</td>\n",
              "      <td>-0.440763</td>\n",
              "      <td>2.278501</td>\n",
              "      <td>-0.380216</td>\n",
              "      <td>-0.413074</td>\n",
              "      <td>-0.388805</td>\n",
              "      <td>-0.259715</td>\n",
              "      <td>-0.297401</td>\n",
              "      <td>-0.334586</td>\n",
              "      <td>-0.274507</td>\n",
              "      <td>-0.309314</td>\n",
              "      <td>-0.321645</td>\n",
              "      <td>-0.308224</td>\n",
              "      <td>-0.243662</td>\n",
              "      <td>-0.291549</td>\n",
              "      <td>-0.332994</td>\n",
              "      <td>2.877096</td>\n",
              "      <td>-0.284696</td>\n",
              "      <td>1.248707</td>\n",
              "      <td>-0.799256</td>\n",
              "      <td>-0.504626</td>\n",
              "      <td>-0.129521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.058741</td>\n",
              "      <td>1.443852</td>\n",
              "      <td>-0.850224</td>\n",
              "      <td>-0.844954</td>\n",
              "      <td>0.844954</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>-0.031204</td>\n",
              "      <td>-0.562357</td>\n",
              "      <td>-0.060972</td>\n",
              "      <td>-0.048218</td>\n",
              "      <td>-0.047827</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>4.614406</td>\n",
              "      <td>-0.026671</td>\n",
              "      <td>-0.078813</td>\n",
              "      <td>-0.034075</td>\n",
              "      <td>-0.013678</td>\n",
              "      <td>-0.161325</td>\n",
              "      <td>-0.040606</td>\n",
              "      <td>-0.017303</td>\n",
              "      <td>-0.280605</td>\n",
              "      <td>-0.163871</td>\n",
              "      <td>-0.051971</td>\n",
              "      <td>-0.3277</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>-0.100449</td>\n",
              "      <td>-0.145214</td>\n",
              "      <td>-0.031799</td>\n",
              "      <td>-0.130562</td>\n",
              "      <td>-0.028041</td>\n",
              "      <td>-0.091308</td>\n",
              "      <td>-0.102332</td>\n",
              "      <td>-0.088774</td>\n",
              "      <td>-0.056482</td>\n",
              "      <td>-0.05375</td>\n",
              "      <td>-0.092549</td>\n",
              "      <td>-0.043723</td>\n",
              "      <td>-0.038226</td>\n",
              "      <td>-0.023696</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044574</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>-0.039195</td>\n",
              "      <td>-0.258482</td>\n",
              "      <td>-0.122944</td>\n",
              "      <td>-0.148931</td>\n",
              "      <td>-0.035688</td>\n",
              "      <td>-0.175652</td>\n",
              "      <td>-0.37815</td>\n",
              "      <td>-0.093164</td>\n",
              "      <td>-0.034075</td>\n",
              "      <td>-0.925440</td>\n",
              "      <td>1.004537</td>\n",
              "      <td>-0.206582</td>\n",
              "      <td>-0.597733</td>\n",
              "      <td>-0.206582</td>\n",
              "      <td>0.661151</td>\n",
              "      <td>-0.393636</td>\n",
              "      <td>-0.400581</td>\n",
              "      <td>2.268794</td>\n",
              "      <td>-0.438885</td>\n",
              "      <td>-0.380216</td>\n",
              "      <td>-0.413074</td>\n",
              "      <td>-0.388805</td>\n",
              "      <td>-0.259715</td>\n",
              "      <td>-0.297401</td>\n",
              "      <td>-0.334586</td>\n",
              "      <td>-0.274507</td>\n",
              "      <td>3.232957</td>\n",
              "      <td>-0.321645</td>\n",
              "      <td>-0.308224</td>\n",
              "      <td>-0.243662</td>\n",
              "      <td>-0.291549</td>\n",
              "      <td>-0.332994</td>\n",
              "      <td>-0.347573</td>\n",
              "      <td>-0.284696</td>\n",
              "      <td>-0.800829</td>\n",
              "      <td>-0.799256</td>\n",
              "      <td>1.981667</td>\n",
              "      <td>-0.129521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.713620</td>\n",
              "      <td>-0.692592</td>\n",
              "      <td>1.200853</td>\n",
              "      <td>1.183497</td>\n",
              "      <td>-1.183497</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>-0.031204</td>\n",
              "      <td>-0.562357</td>\n",
              "      <td>-0.060972</td>\n",
              "      <td>-0.048218</td>\n",
              "      <td>-0.047827</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>-0.216713</td>\n",
              "      <td>37.493859</td>\n",
              "      <td>-0.078813</td>\n",
              "      <td>-0.034075</td>\n",
              "      <td>-0.013678</td>\n",
              "      <td>-0.161325</td>\n",
              "      <td>-0.040606</td>\n",
              "      <td>-0.017303</td>\n",
              "      <td>-0.280605</td>\n",
              "      <td>-0.163871</td>\n",
              "      <td>-0.051971</td>\n",
              "      <td>-0.3277</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>-0.100449</td>\n",
              "      <td>-0.145214</td>\n",
              "      <td>-0.031799</td>\n",
              "      <td>-0.130562</td>\n",
              "      <td>-0.028041</td>\n",
              "      <td>-0.091308</td>\n",
              "      <td>-0.102332</td>\n",
              "      <td>-0.088774</td>\n",
              "      <td>-0.056482</td>\n",
              "      <td>-0.05375</td>\n",
              "      <td>-0.092549</td>\n",
              "      <td>-0.043723</td>\n",
              "      <td>-0.038226</td>\n",
              "      <td>-0.023696</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044574</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>-0.039195</td>\n",
              "      <td>-0.258482</td>\n",
              "      <td>-0.122944</td>\n",
              "      <td>-0.148931</td>\n",
              "      <td>-0.035688</td>\n",
              "      <td>-0.175652</td>\n",
              "      <td>-0.37815</td>\n",
              "      <td>-0.093164</td>\n",
              "      <td>-0.034075</td>\n",
              "      <td>-0.925440</td>\n",
              "      <td>1.004537</td>\n",
              "      <td>-0.206582</td>\n",
              "      <td>1.672989</td>\n",
              "      <td>-0.206582</td>\n",
              "      <td>-1.512514</td>\n",
              "      <td>2.540415</td>\n",
              "      <td>-0.400581</td>\n",
              "      <td>-0.440763</td>\n",
              "      <td>-0.438885</td>\n",
              "      <td>-0.380216</td>\n",
              "      <td>-0.413074</td>\n",
              "      <td>-0.388805</td>\n",
              "      <td>-0.259715</td>\n",
              "      <td>-0.297401</td>\n",
              "      <td>-0.334586</td>\n",
              "      <td>-0.274507</td>\n",
              "      <td>-0.309314</td>\n",
              "      <td>3.109019</td>\n",
              "      <td>-0.308224</td>\n",
              "      <td>-0.243662</td>\n",
              "      <td>-0.291549</td>\n",
              "      <td>-0.332994</td>\n",
              "      <td>-0.347573</td>\n",
              "      <td>-0.284696</td>\n",
              "      <td>-0.800829</td>\n",
              "      <td>1.251163</td>\n",
              "      <td>-0.504626</td>\n",
              "      <td>-0.129521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.058741</td>\n",
              "      <td>-0.692592</td>\n",
              "      <td>-0.557213</td>\n",
              "      <td>-0.844954</td>\n",
              "      <td>0.844954</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>-0.031204</td>\n",
              "      <td>-0.562357</td>\n",
              "      <td>-0.060972</td>\n",
              "      <td>-0.048218</td>\n",
              "      <td>-0.047827</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>-0.216713</td>\n",
              "      <td>-0.026671</td>\n",
              "      <td>-0.078813</td>\n",
              "      <td>-0.034075</td>\n",
              "      <td>-0.013678</td>\n",
              "      <td>-0.161325</td>\n",
              "      <td>-0.040606</td>\n",
              "      <td>-0.017303</td>\n",
              "      <td>-0.280605</td>\n",
              "      <td>-0.163871</td>\n",
              "      <td>-0.051971</td>\n",
              "      <td>-0.3277</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>-0.100449</td>\n",
              "      <td>-0.145214</td>\n",
              "      <td>-0.031799</td>\n",
              "      <td>-0.130562</td>\n",
              "      <td>-0.028041</td>\n",
              "      <td>-0.091308</td>\n",
              "      <td>-0.102332</td>\n",
              "      <td>-0.088774</td>\n",
              "      <td>-0.056482</td>\n",
              "      <td>-0.05375</td>\n",
              "      <td>-0.092549</td>\n",
              "      <td>-0.043723</td>\n",
              "      <td>-0.038226</td>\n",
              "      <td>-0.023696</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044574</td>\n",
              "      <td>-0.00865</td>\n",
              "      <td>-0.039195</td>\n",
              "      <td>3.868739</td>\n",
              "      <td>-0.122944</td>\n",
              "      <td>-0.148931</td>\n",
              "      <td>-0.035688</td>\n",
              "      <td>-0.175652</td>\n",
              "      <td>-0.37815</td>\n",
              "      <td>-0.093164</td>\n",
              "      <td>-0.034075</td>\n",
              "      <td>-0.925440</td>\n",
              "      <td>1.004537</td>\n",
              "      <td>-0.206582</td>\n",
              "      <td>-0.597733</td>\n",
              "      <td>-0.206582</td>\n",
              "      <td>0.661151</td>\n",
              "      <td>2.540415</td>\n",
              "      <td>-0.400581</td>\n",
              "      <td>-0.440763</td>\n",
              "      <td>-0.438885</td>\n",
              "      <td>-0.380216</td>\n",
              "      <td>-0.413074</td>\n",
              "      <td>-0.388805</td>\n",
              "      <td>-0.259715</td>\n",
              "      <td>-0.297401</td>\n",
              "      <td>-0.334586</td>\n",
              "      <td>-0.274507</td>\n",
              "      <td>-0.309314</td>\n",
              "      <td>-0.321645</td>\n",
              "      <td>-0.308224</td>\n",
              "      <td>-0.243662</td>\n",
              "      <td>-0.291549</td>\n",
              "      <td>3.003059</td>\n",
              "      <td>-0.347573</td>\n",
              "      <td>-0.284696</td>\n",
              "      <td>1.248707</td>\n",
              "      <td>-0.799256</td>\n",
              "      <td>-0.504626</td>\n",
              "      <td>-0.129521</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 91 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        88        89        90\n",
              "0 -0.395879 -0.692592  0.907842  ...  1.251163 -0.504626 -0.129521\n",
              "1 -0.395879  1.443852 -0.557213  ... -0.799256 -0.504626 -0.129521\n",
              "2 -0.058741  1.443852 -0.850224  ... -0.799256  1.981667 -0.129521\n",
              "3 -0.713620 -0.692592  1.200853  ...  1.251163 -0.504626 -0.129521\n",
              "4 -0.058741 -0.692592 -0.557213  ... -0.799256 -0.504626 -0.129521\n",
              "\n",
              "[5 rows x 91 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3wDpIf4qBj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)\n",
        "y_train_unaltered = y_train\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.fit_transform(y_test)\n",
        "y_val = lb.fit_transform(y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNpx_sX8rp7b",
        "colab_type": "code",
        "outputId": "a89d7e21-8629-4712-9dc1-94421cc5de8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(20, input_shape=(x_train.shape[1],), activation='relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(20, activation='relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(20, activation='relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(20, activation='relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(5, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_11 (Dense)             (None, 20)                1840      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 5)                 105       \n",
            "=================================================================\n",
            "Total params: 3,205\n",
            "Trainable params: 3,205\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4WA8tgWrsxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4a3jVZPyhzl",
        "colab_type": "code",
        "outputId": "f76a3e27-637c-41e0-a8b8-513f52b6ebd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs=30, batch_size=16, validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 17106 samples, validate on 4277 samples\n",
            "Epoch 1/30\n",
            "17106/17106 [==============================] - 2s 138us/step - loss: 1.0987 - acc: 0.5616 - val_loss: 0.9590 - val_acc: 0.6102\n",
            "Epoch 2/30\n",
            "17106/17106 [==============================] - 2s 100us/step - loss: 0.9726 - acc: 0.6238 - val_loss: 0.9220 - val_acc: 0.6346\n",
            "Epoch 3/30\n",
            "17106/17106 [==============================] - 2s 100us/step - loss: 0.9463 - acc: 0.6322 - val_loss: 0.9127 - val_acc: 0.6369\n",
            "Epoch 4/30\n",
            "17106/17106 [==============================] - 2s 101us/step - loss: 0.9282 - acc: 0.6371 - val_loss: 0.9079 - val_acc: 0.6346\n",
            "Epoch 5/30\n",
            "17106/17106 [==============================] - 2s 102us/step - loss: 0.9157 - acc: 0.6408 - val_loss: 0.8978 - val_acc: 0.6348\n",
            "Epoch 6/30\n",
            "17106/17106 [==============================] - 2s 100us/step - loss: 0.9050 - acc: 0.6440 - val_loss: 0.9047 - val_acc: 0.6362\n",
            "Epoch 7/30\n",
            "17106/17106 [==============================] - 2s 102us/step - loss: 0.8995 - acc: 0.6447 - val_loss: 0.8954 - val_acc: 0.6376\n",
            "Epoch 8/30\n",
            "17106/17106 [==============================] - 2s 103us/step - loss: 0.8947 - acc: 0.6488 - val_loss: 0.8940 - val_acc: 0.6360\n",
            "Epoch 9/30\n",
            "17106/17106 [==============================] - 2s 101us/step - loss: 0.8925 - acc: 0.6468 - val_loss: 0.8937 - val_acc: 0.6341\n",
            "Epoch 10/30\n",
            "17106/17106 [==============================] - 2s 100us/step - loss: 0.8859 - acc: 0.6511 - val_loss: 0.8929 - val_acc: 0.6427\n",
            "Epoch 11/30\n",
            "17106/17106 [==============================] - 2s 101us/step - loss: 0.8819 - acc: 0.6504 - val_loss: 0.8916 - val_acc: 0.6364\n",
            "Epoch 12/30\n",
            "17106/17106 [==============================] - 2s 102us/step - loss: 0.8791 - acc: 0.6554 - val_loss: 0.8909 - val_acc: 0.6427\n",
            "Epoch 13/30\n",
            "17106/17106 [==============================] - 2s 103us/step - loss: 0.8739 - acc: 0.6544 - val_loss: 0.8910 - val_acc: 0.6434\n",
            "Epoch 14/30\n",
            "17106/17106 [==============================] - 2s 103us/step - loss: 0.8726 - acc: 0.6549 - val_loss: 0.8903 - val_acc: 0.6374\n",
            "Epoch 15/30\n",
            "17106/17106 [==============================] - 2s 103us/step - loss: 0.8713 - acc: 0.6553 - val_loss: 0.8877 - val_acc: 0.6411\n",
            "Epoch 16/30\n",
            "17106/17106 [==============================] - 2s 103us/step - loss: 0.8686 - acc: 0.6575 - val_loss: 0.8874 - val_acc: 0.6423\n",
            "Epoch 17/30\n",
            "17106/17106 [==============================] - 2s 103us/step - loss: 0.8693 - acc: 0.6577 - val_loss: 0.8891 - val_acc: 0.6385\n",
            "Epoch 18/30\n",
            "17106/17106 [==============================] - 2s 103us/step - loss: 0.8632 - acc: 0.6564 - val_loss: 0.8860 - val_acc: 0.6369\n",
            "Epoch 19/30\n",
            "17106/17106 [==============================] - 2s 103us/step - loss: 0.8637 - acc: 0.6585 - val_loss: 0.8834 - val_acc: 0.6420\n",
            "Epoch 20/30\n",
            "17106/17106 [==============================] - 2s 102us/step - loss: 0.8624 - acc: 0.6592 - val_loss: 0.8819 - val_acc: 0.6434\n",
            "Epoch 21/30\n",
            "17106/17106 [==============================] - 2s 103us/step - loss: 0.8616 - acc: 0.6608 - val_loss: 0.8812 - val_acc: 0.6427\n",
            "Epoch 22/30\n",
            "17106/17106 [==============================] - 2s 104us/step - loss: 0.8550 - acc: 0.6620 - val_loss: 0.8839 - val_acc: 0.6423\n",
            "Epoch 23/30\n",
            "17106/17106 [==============================] - 2s 105us/step - loss: 0.8593 - acc: 0.6581 - val_loss: 0.8854 - val_acc: 0.6409\n",
            "Epoch 24/30\n",
            "17106/17106 [==============================] - 2s 103us/step - loss: 0.8567 - acc: 0.6595 - val_loss: 0.8857 - val_acc: 0.6397\n",
            "Epoch 25/30\n",
            "17106/17106 [==============================] - 2s 105us/step - loss: 0.8543 - acc: 0.6616 - val_loss: 0.8838 - val_acc: 0.6390\n",
            "Epoch 26/30\n",
            "17106/17106 [==============================] - 2s 103us/step - loss: 0.8551 - acc: 0.6610 - val_loss: 0.8842 - val_acc: 0.6432\n",
            "Epoch 27/30\n",
            "17106/17106 [==============================] - 2s 103us/step - loss: 0.8515 - acc: 0.6625 - val_loss: 0.8823 - val_acc: 0.6406\n",
            "Epoch 28/30\n",
            "17106/17106 [==============================] - 2s 104us/step - loss: 0.8521 - acc: 0.6631 - val_loss: 0.8866 - val_acc: 0.6399\n",
            "Epoch 29/30\n",
            "17106/17106 [==============================] - 2s 103us/step - loss: 0.8519 - acc: 0.6640 - val_loss: 0.8837 - val_acc: 0.6413\n",
            "Epoch 30/30\n",
            "17106/17106 [==============================] - 2s 103us/step - loss: 0.8547 - acc: 0.6611 - val_loss: 0.8871 - val_acc: 0.6444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TarV8UWL0Ne-",
        "colab_type": "code",
        "outputId": "1f3011d0-5e3a-4e5f-8105-1fd531afc961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYCEiIgkpWIiISEL/H/BV6kmOA69iAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPNYWQY76sps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numericLabels(x):\n",
        "    outputArr = np.zeros((len(x), 5))\n",
        "    \n",
        "    for i in range(len(x)):\n",
        "        outputArr[i, x[i]] = 1\n",
        "                \n",
        "    return outputArr\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGPoHL9l_Jgy",
        "colab_type": "code",
        "outputId": "931c3cfc-66cd-424d-b084-0977ae297a36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5346, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF4c7wCQ0O9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict_classes(x_test)\n",
        "prob = model.predict(x_test)\n",
        "pred = numericLabels(pred)\n",
        "print(accuracy_score(pred, y_test))\n",
        "print(log_loss(y_test, prob))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WyemvoYLxgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gb = GradientBoostingClassifier(n_estimators=30, learning_rate=0.5)\n",
        "gb.fit(x_train, y_train_unaltered)\n",
        "gbprob = gb.predict_proba(x_test)\n",
        "log_loss(y_test, gbprob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG8o-ViCaNK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors = 133)\n",
        "knn.fit(x_train, y_train_unaltered)\n",
        "knnprob = knn.predict_proba(x_test)\n",
        "log_loss(y_test, knnprob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKzjsP8RiM2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=900)\n",
        "rf.fit(x_train, y_train_unaltered)\n",
        "rfprob = rf.predict_proba(x_test)\n",
        "log_loss(y_test, rfprob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8LuS2CDjBeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm = SVC(kernel='rbf', probability=True)\n",
        "svm.fit(x_train, y_train_unaltered)\n",
        "svmprob = svm.predict_proba(x_test)\n",
        "log_loss(y_test, svmprob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WS2hOdEjefh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}